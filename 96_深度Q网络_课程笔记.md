### 深度Q网络

#### 深度Q网络

直观想法

- 使用神经网络逼近$Q_(\Theta)(s, a)$
- 算法不稳定

解决办法

- 经验回放
- 使用双网络结构：评估网络和目标网络

#### 经验回放

存储训练过程中的每一步到数据库中，采样时服从均匀分布

优先经验回放

- 衡量标准
- 选中的概率
- 重要性采样

#### 双网络结构

目标网络$Q_(\Theta)(s, a)$

- 使用较旧的参数，每个C步和训练网络的参数同步一次

#### 算法流程

1. 收集数据：使用$\epsilon - greedy$策略进行探索，将得到的状态动作组放入经验池（replay-buffer）
2. 采样：从数据库中采样k个动作状态组
3. 更新网络

#### 在Atari环境中的实验结果



#### double DQN

为了解决DQN的过高估计和有时候Q值很大

max操作会是的Q函数的值越来越大，甚至高于真实值

double DQN使用不同的网络来估值和决策

#### 过高估计的例子与在Atari环境中的实验结果



#### Dueling DQN

#### 网络结构

#### 优点

- 处理与动作关联较小的状态
- 状态值函数的学习较为有效：一个状态值函数对应多个advantage函数

#### 在Atari环境中的实验结果