### Actor-Critic

基于价值函数和策略梯度的算法

#### REINFORCE存在的问题

- 高训练方差（最重要的缺陷）
- 低数据利用率
- 基于片段式数据的任务

#### Actor-Critic思想

REINFORCEC策略梯度方法是使用蒙特卡洛蔡阳直接$(s_t, a_t)$的值$G_t$

为什么不建立一个可训练的值函数$Q_(\Phi)$来完成这个估计过程？

 演员（$(\Pi)_(\theta)(a \mid s)$）：采取动作是评论家满意的策略

评论家（$Q_(\Phi)(s, a)$）：学会准确估计演员策略所采取的动作价值的值函数

#### Actor-Critic训练

评论家

- 负责学会准确估计当前演员策略（actor policy）的动作价值

演员

- 学会采取使评论家满意的动作

#### A2C: Advantageous Actor-Critic

思想

- 通过减去一个基线函数来标准化评论家的打分

- 更多信息指导，降低较差动作概率，提高交较优动作概率

优势函数（Advantageous Function）

状态-动作值和状态值函数

拟合状态值函数来拟合优势函数

